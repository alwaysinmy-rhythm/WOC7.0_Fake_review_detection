{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv and call reviews in pd\n",
    "df = pd.read_csv(\"../fake reviews dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Normalization --> make lower case , remove punctuations , special character, numbers\n",
    "\n",
    "df['text_'] =df['text_'].str.lower()\n",
    "\n",
    "\n",
    "#incase of web scrapping will need to remove html tags code for that\n",
    "# import re\n",
    "# def remove_html_tags(text):\n",
    "#   pattern = re.compile('<.*?>')\n",
    "#   return pattern.sub(r'' , text)\n",
    "# df['text_'] = df['text_'].apply(remove_html_tags)\n",
    "\n",
    "exclude = string.punctuation\n",
    "\n",
    "# def remove_punc(text):\n",
    "#   for char in exclude :\n",
    "#     text = text.replace(char, '')\n",
    "#   return text\n",
    "#very slow\n",
    "\n",
    "def remove_punc(text):\n",
    "  return text.translate(str.maketrans('', '' , exclude))\n",
    "\n",
    "\n",
    "df['text_'] = df['text_'].apply(remove_punc)\n",
    "\n",
    "#spelling correction\n",
    "# from textblob import TextBlob\n",
    "# textBlb = TextBlob(df['text_'])\n",
    "# df['text_'] = textBlb.correct().string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword removal --> eliminating common words\n",
    "\n",
    "# print(stopwords.words('english'))\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# print(stopwords)\n",
    "# def remove_stopwords(text):\n",
    "\n",
    "#   new_text = []\n",
    "\n",
    "#   for word in text.split():\n",
    "#     if word in stopwords.words('english'):\n",
    "#       new_text.append('')\n",
    "#     else :\n",
    "#       new_text.append(word)\n",
    "#   x = new_text[:]\n",
    "#   new_text.clear()\n",
    "#   return \" \".join(x)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Replace stopwords with an empty string but preserve spaces\n",
    "    return \" \".join([word if word not in stop_words else '' for word in text.split()])\n",
    "\n",
    "\n",
    "\n",
    "df['text_'] = df['text_'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization --> break the review in words\n",
    "# import spacy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# def give_tokens(text):\n",
    "#   tokens = []\n",
    "#   doc = nlp(text)\n",
    "#   for token in doc:\n",
    "#     tokens.append(token)\n",
    "#   return doc\n",
    "\n",
    "# df['text_'] = df['text_'].apply(give_tokens)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def give_token(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "# str1 = \"my name is rhythm\"\n",
    "# word_tokenize(str1)\n",
    "\n",
    "df['text_'] = df['text_'].apply(give_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #stemming --> eg running -> run\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "# def stem_words(wordlist):\n",
    "#   text = []\n",
    "#   for word in wordlist :\n",
    "#     text.append(ps.stem(word))\n",
    "#   return text\n",
    "\n",
    "# df['text_'] = df['text_'].apply(stem_words)\n",
    "# df.head()\n",
    "# # def give(text):\n",
    "# #   return ps.stem(text)\n",
    "\n",
    "# # give(\"walking\")\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "df['text_'] = df['text_'].apply(lemmatize_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.models import Word2Vec,KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin',binary=True,limit=500000)\n",
    "import pickle \n",
    "with open(\"../vectorize.pkl\", \"rb\") as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_16884\\2399057821.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[i] = ['x']\n"
     ]
    }
   ],
   "source": [
    "x = df['text_']\n",
    "for i in range(len(x)):\n",
    "    if( len(x[i]) == 0):\n",
    "        x[i] = ['x']\n",
    "\n",
    "\n",
    "\n",
    "def get_vector(text):\n",
    "    return model.get_mean_vector(text)\n",
    "\n",
    "df['text_'] = df['text_'].apply(get_vector)\n",
    "\n",
    "x = np.vstack(df['text_'].values)\n",
    "y = np.array(df['label'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#   \n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Creating an SVM classifier\n",
    "clf = svm.SVC(kernel='rbf', C=25 )  # You can use 'rbf', 'poly', or 'sigmoid' kernels as well\n",
    "\n",
    "# Training the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Making predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"../review_model.pkl\", \"wb\") as file: \n",
    "    pickle.dump(clf, file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
